"""\nIncremental Scraping Engine\nImplements change detection using embedding-based similarity scores and intelligent caching.\n"""\n\nimport os\nimport json\nimport asyncio\nimport hashlib\nimport time\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nimport logging\nfrom urllib.parse import urlparse\nimport numpy as np\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import and_\n\nfrom intelligent_analyzer import IntelligentAnalyzer\nfrom models import ScrapingJob, ScrapingResult, ContentFingerprint\nfrom database import get_db\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass ChangeDetectionResult:\n    \"\"\"Result from change detection analysis\"\"\"\n    has_changed: bool\n    similarity_score: float\n    change_type: str  # 'content', 'structure', 'semantic', 'none'\n    changed_sections: List[str] = field(default_factory=list)\n    confidence: float = 0.0\n    previous_version_id: Optional[str] = None\n    change_summary: str = \"\"\n\n@dataclass\nclass CacheEntry:\n    \"\"\"Cache entry for scraped content\"\"\"\n    url: str\n    content_hash: str\n    semantic_embedding: List[float]\n    last_scraped: datetime\n    scraping_job_id: str\n    content_fingerprint: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\nclass IncrementalScrapingEngine:\n    \"\"\"Engine for incremental scraping with intelligent change detection\"\"\"\n\n    def __init__(self, similarity_threshold: float = 0.85):\n        self.analyzer = IntelligentAnalyzer()\n        self.similarity_threshold = similarity_threshold\n        self.cache: Dict[str, CacheEntry] = {}\n        self.embedding_cache: Dict[str, List[float]] = {}\n        \n        # Change detection configurations\n        self.change_detection_config = {\n            'content_weight': 0.4,\n            'structure_weight': 0.3,\n            'semantic_weight': 0.3,\n            'min_change_threshold': 0.1,\n            'significant_change_threshold': 0.3\n        }\n\n    async def should_scrape(\n        self,\n        url: str,\n        force_refresh: bool = False,\n        max_age_hours: int = 24\n    ) -> Tuple[bool, Optional[ChangeDetectionResult]]:\n        \"\"\"\n        Determine if a URL should be scraped based on change detection\n        \"\"\"\n        if force_refresh:\n            return True, None\n\n        # Check cache for existing entry\n        cache_entry = await self._get_cache_entry(url)\n        if not cache_entry:\n            return True, None\n\n        # Check age-based refresh\n        age_hours = (datetime.now() - cache_entry.last_scraped).total_seconds() / 3600\n        if age_hours > max_age_hours:\n            logger.info(f\"Content for {url} is {age_hours:.1f} hours old, refreshing\")\n            return True, None\n\n        # Perform lightweight change detection\n        try:\n            change_result = await self._detect_changes_lightweight(url, cache_entry)\n            \n            if change_result.has_changed:\n                logger.info(f\"Changes detected for {url}: {change_result.change_type} (similarity: {change_result.similarity_score:.3f})\")\n                return True, change_result\n            else:\n                logger.info(f\"No significant changes detected for {url} (similarity: {change_result.similarity_score:.3f})\")\n                return False, change_result\n\n        except Exception as e:\n            logger.warning(f\"Error in change detection for {url}: {str(e)}\")\n            return True, None\n\n    async def update_cache(\n        self,\n        url: str,\n        content: str,\n        scraping_job_id: str,\n        metadata: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"\n        Update cache with new content and generate embeddings\n        \"\"\"\n        try:\n            # Generate content hash\n            content_hash = hashlib.sha256(content.encode()).hexdigest()\n            \n            # Generate semantic embedding\n            semantic_embedding = await self.analyzer.generate_content_embedding(content)\n            \n            # Generate content fingerprint\n            fingerprint = await self._generate_content_fingerprint(content)\n            \n            # Create cache entry\n            cache_entry = CacheEntry(\n                url=url,\n                content_hash=content_hash,\n                semantic_embedding=semantic_embedding,\n                last_scraped=datetime.now(),\n                scraping_job_id=scraping_job_id,\n                content_fingerprint=fingerprint,\n                metadata=metadata or {}\n            )\n            \n            # Update in-memory cache\n            self.cache[url] = cache_entry\n            \n            # Store in database\n            await self._store_cache_entry(cache_entry)\n            \n            logger.info(f\"Cache updated for {url}\")\n\n        except Exception as e:\n            logger.error(f\"Error updating cache for {url}: {str(e)}\")\n\n    async def _get_cache_entry(self, url: str) -> Optional[CacheEntry]:\n        \"\"\"Get cache entry from memory or database\"\"\"\n        # Check in-memory cache first\n        if url in self.cache:\n            return self.cache[url]\n\n        # Check database\n        try:\n            db = next(get_db())\n            fingerprint = db.query(ContentFingerprint).filter(\n                ContentFingerprint.url == url\n            ).order_by(ContentFingerprint.created_at.desc()).first()\n            \n            if fingerprint:\n                cache_entry = CacheEntry(\n                    url=url,\n                    content_hash=fingerprint.content_hash,\n                    semantic_embedding=json.loads(fingerprint.semantic_embedding),\n                    last_scraped=fingerprint.created_at,\n                    scraping_job_id=str(fingerprint.job_id),\n                    content_fingerprint=fingerprint.fingerprint,\n                    metadata=json.loads(fingerprint.metadata) if fingerprint.metadata else {}\n                )\n                \n                # Cache in memory\n                self.cache[url] = cache_entry\n                return cache_entry\n\n        except Exception as e:\n            logger.error(f\"Error retrieving cache entry for {url}: {str(e)}\")\n\n        return None\n\n    async def _detect_changes_lightweight(\n        self,\n        url: str,\n        cache_entry: CacheEntry\n    ) -> ChangeDetectionResult:\n        \"\"\"\n        Perform lightweight change detection without full scraping\n        \"\"\"\n        try:\n            # Quick content analysis\n            current_analysis = await self.analyzer.analyze_page_lightweight(url)\n            \n            # Generate current content fingerprint\n            current_fingerprint = await self._generate_content_fingerprint(current_analysis.get('content', ''))\n            \n            # Compare fingerprints\n            fingerprint_similarity = self._compare_fingerprints(\n                cache_entry.content_fingerprint,\n                current_fingerprint\n            )\n            \n            # Generate semantic embedding for current content\n            current_embedding = await self.analyzer.generate_content_embedding(\n                current_analysis.get('content', '')\n            )\n            \n            # Calculate semantic similarity\n            semantic_similarity = self._calculate_cosine_similarity(\n                cache_entry.semantic_embedding,\n                current_embedding\n            )\n            \n            # Determine overall similarity\n            overall_similarity = (\n                fingerprint_similarity * self.change_detection_config['content_weight'] +\n                semantic_similarity * self.change_detection_config['semantic_weight']\n            )\n            \n            # Determine change type and significance\n            has_changed = overall_similarity < self.similarity_threshold\n            change_type = self._determine_change_type(\n                fingerprint_similarity,\n                semantic_similarity,\n                current_analysis\n            )\n            \n            return ChangeDetectionResult(\n                has_changed=has_changed,\n                similarity_score=overall_similarity,\n                change_type=change_type,\n                confidence=min(abs(overall_similarity - self.similarity_threshold) * 2, 1.0),\n                previous_version_id=cache_entry.scraping_job_id,\n                change_summary=self._generate_change_summary(\n                    change_type, overall_similarity, fingerprint_similarity, semantic_similarity\n                )\n            )\n\n        except Exception as e:\n            logger.error(f\"Error in lightweight change detection: {str(e)}\")\n            return ChangeDetectionResult(\n                has_changed=True,\n                similarity_score=0.0,\n                change_type='error',\n                confidence=0.0,\n                change_summary=f\"Error in change detection: {str(e)}\"\n            )\n\n    async def _generate_content_fingerprint(self, content: str) -> str:\n        \"\"\"Generate a structural fingerprint of content\"\"\"\n        try:\n            # Extract key structural elements\n            from bs4 import BeautifulSoup\n            soup = BeautifulSoup(content, 'html.parser')\n            \n            # Count different element types\n            element_counts = {}\n            for tag in soup.find_all():\n                element_counts[tag.name] = element_counts.get(tag.name, 0) + 1\n            \n            # Extract text length and word count\n            text_content = soup.get_text()\n            text_length = len(text_content)\n            word_count = len(text_content.split())\n            \n            # Create fingerprint\n            fingerprint_data = {\n                'element_counts': element_counts,\n                'text_length': text_length,\n                'word_count': word_count,\n                'title': soup.title.string if soup.title else '',\n                'meta_count': len(soup.find_all('meta')),\n                'link_count': len(soup.find_all('a')),\n                'image_count': len(soup.find_all('img'))\n            }\n            \n            # Generate hash of fingerprint\n            fingerprint_str = json.dumps(fingerprint_data, sort_keys=True)\n            return hashlib.md5(fingerprint_str.encode()).hexdigest()\n\n        except Exception as e:\n            logger.warning(f\"Error generating content fingerprint: {str(e)}\")\n            return hashlib.md5(content.encode()).hexdigest()\n\n    def _compare_fingerprints(self, fp1: str, fp2: str) -> float:\n        \"\"\"Compare two content fingerprints\"\"\"\n        if fp1 == fp2:\n            return 1.0\n        return 0.0\n\n    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n        try:\n            vec1_np = np.array(vec1)\n            vec2_np = np.array(vec2)\n            \n            dot_product = np.dot(vec1_np, vec2_np)\n            norm1 = np.linalg.norm(vec1_np)\n            norm2 = np.linalg.norm(vec2_np)\n            \n            if norm1 == 0 or norm2 == 0:\n                return 0.0\n            \n            return dot_product / (norm1 * norm2)\n\n        except Exception as e:\n            logger.warning(f\"Error calculating cosine similarity: {str(e)}\")\n            return 0.0\n\n    def _determine_change_type(\n        self,\n        fingerprint_similarity: float,\n        semantic_similarity: float,\n        current_analysis: Dict[str, Any]\n    ) -> str:\n        \"\"\"Determine the type of change detected\"\"\"\n        if fingerprint_similarity < 0.5:\n            return 'structure'\n        elif semantic_similarity < 0.7:\n            return 'semantic'\n        elif fingerprint_similarity < 0.9 or semantic_similarity < 0.9:\n            return 'content'\n        else:\n            return 'none'\n\n    def _generate_change_summary(\n        self,\n        change_type: str,\n        overall_similarity: float,\n        fingerprint_similarity: float,\n        semantic_similarity: float\n    ) -> str:\n        \"\"\"Generate a human-readable summary of changes\"\"\"\n        if change_type == 'none':\n            return \"No significant changes detected\"\n        elif change_type == 'structure':\n            return f\"Structural changes detected (similarity: {fingerprint_similarity:.2f})\"\n        elif change_type == 'semantic':\n            return f\"Semantic changes detected (similarity: {semantic_similarity:.2f})\"\n        elif change_type == 'content':\n            return f\"Content changes detected (overall similarity: {overall_similarity:.2f})\"\n        else:\n            return f\"Changes detected ({change_type})\"\n\n    async def _store_cache_entry(self, cache_entry: CacheEntry):\n        \"\"\"Store cache entry in database\"\"\"\n        try:\n            db = next(get_db())\n            \n            fingerprint = ContentFingerprint(\n                job_id=int(cache_entry.scraping_job_id),\n                url=cache_entry.url,\n                content_hash=cache_entry.content_hash,\n                fingerprint=cache_entry.content_fingerprint,\n                semantic_embedding=json.dumps(cache_entry.semantic_embedding),\n                metadata=json.dumps(cache_entry.metadata)\n            )\n            \n            db.add(fingerprint)\n            db.commit()\n\n        except Exception as e:\n            logger.error(f\"Error storing cache entry: {str(e)}\")\n\n    async def get_change_history(self, url: str, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get change history for a URL\"\"\"\n        try:\n            db = next(get_db())\n            fingerprints = db.query(ContentFingerprint).filter(\n                ContentFingerprint.url == url\n            ).order_by(ContentFingerprint.created_at.desc()).limit(limit).all()\n            \n            history = []\n            for fp in fingerprints:\n                history.append({\n                    'timestamp': fp.created_at,\n                    'job_id': fp.job_id,\n                    'content_hash': fp.content_hash,\n                    'fingerprint': fp.fingerprint,\n                    'metadata': json.loads(fp.metadata) if fp.metadata else {}\n                })\n            \n            return history\n\n        except Exception as e:\n            logger.error(f\"Error retrieving change history for {url}: {str(e)}\")\n            return []\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics\"\"\"\n        return {\n            'cached_urls': len(self.cache),\n            'similarity_threshold': self.similarity_threshold,\n            'config': self.change_detection_config\n        }\n"}}}