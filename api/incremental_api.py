"""\nIncremental Scraping API Endpoints\nProvides endpoints for managing incremental scraping and change detection.\n"""\n\nfrom fastapi import APIRouter, HTTPException, Depends, Query\nfrom pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom sqlalchemy.orm import Session\n\nfrom database import get_db\nfrom incremental_scraper import IncrementalScrapingEngine\nimport models\n\nrouter = APIRouter(prefix="/incremental", tags=["incremental"])\n\n# Initialize incremental scraper\nincremental_scraper = IncrementalScrapingEngine()\n\n# Pydantic models\nclass ChangeDetectionRequest(BaseModel):\n    url: str\n    max_age_hours: int = 24\n\nclass ChangeDetectionResponse(BaseModel):\n    should_scrape: bool\n    has_changed: bool\n    similarity_score: float\n    change_type: str\n    change_summary: str\n    confidence: float\n    last_scraped: Optional[datetime] = None\n\nclass CacheStatsResponse(BaseModel):\n    cached_urls: int\n    similarity_threshold: float\n    config: Dict[str, Any]\n\nclass ChangeHistoryItem(BaseModel):\n    timestamp: datetime\n    job_id: int\n    content_hash: str\n    fingerprint: str\n    metadata: Dict[str, Any]\n\nclass ChangeHistoryResponse(BaseModel):\n    url: str\n    history: List[ChangeHistoryItem]\n\n@router.post("/check-changes", response_model=ChangeDetectionResponse)\nasync def check_changes(request: ChangeDetectionRequest):\n    \"\"\"\n    Check if a URL has changes and should be scraped\n    \"\"\"\n    try:\n        should_scrape, change_result = await incremental_scraper.should_scrape(\n            request.url,\n            force_refresh=False,\n            max_age_hours=request.max_age_hours\n        )\n        \n        if change_result:\n            return ChangeDetectionResponse(\n                should_scrape=should_scrape,\n                has_changed=change_result.has_changed,\n                similarity_score=change_result.similarity_score,\n                change_type=change_result.change_type,\n                change_summary=change_result.change_summary,\n                confidence=change_result.confidence\n            )\n        else:\n            return ChangeDetectionResponse(\n                should_scrape=should_scrape,\n                has_changed=True,\n                similarity_score=0.0,\n                change_type=\"new\",\n                change_summary=\"No previous version found\",\n                confidence=1.0\n            )\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error checking changes: {str(e)}\")\n\n@router.get(\"/cache-stats\", response_model=CacheStatsResponse)\nasync def get_cache_stats():\n    \"\"\"\n    Get incremental scraping cache statistics\n    \"\"\"\n    try:\n        stats = incremental_scraper.get_cache_stats()\n        return CacheStatsResponse(**stats)\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error getting cache stats: {str(e)}\")\n\n@router.get(\"/change-history/{url:path}\", response_model=ChangeHistoryResponse)\nasync def get_change_history(\n    url: str,\n    limit: int = Query(10, ge=1, le=100)\n):\n    \"\"\"\n    Get change history for a specific URL\n    \"\"\"\n    try:\n        history = await incremental_scraper.get_change_history(url, limit)\n        \n        history_items = [\n            ChangeHistoryItem(\n                timestamp=item['timestamp'],\n                job_id=item['job_id'],\n                content_hash=item['content_hash'],\n                fingerprint=item['fingerprint'],\n                metadata=item['metadata']\n            )\n            for item in history\n        ]\n        \n        return ChangeHistoryResponse(\n            url=url,\n            history=history_items\n        )\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error getting change history: {str(e)}\")\n\n@router.delete(\"/cache/{url:path}\")\nasync def clear_cache_for_url(url: str, db: Session = Depends(get_db)):\n    \"\"\"\n    Clear cache for a specific URL\n    \"\"\"\n    try:\n        # Remove from in-memory cache\n        if url in incremental_scraper.cache:\n            del incremental_scraper.cache[url]\n        \n        # Remove from database\n        db.query(models.ContentFingerprint).filter(\n            models.ContentFingerprint.url == url\n        ).delete()\n        db.commit()\n        \n        return {\"message\": f\"Cache cleared for {url}\"}\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error clearing cache: {str(e)}\")\n\n@router.delete(\"/cache\")\nasync def clear_all_cache(db: Session = Depends(get_db)):\n    \"\"\"\n    Clear all incremental scraping cache\n    \"\"\"\n    try:\n        # Clear in-memory cache\n        incremental_scraper.cache.clear()\n        incremental_scraper.embedding_cache.clear()\n        \n        # Clear database cache\n        db.query(models.ContentFingerprint).delete()\n        db.commit()\n        \n        return {\"message\": \"All cache cleared\"}\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error clearing all cache: {str(e)}\")\n\n@router.put(\"/config\")\nasync def update_config(\n    similarity_threshold: Optional[float] = Query(None, ge=0.0, le=1.0),\n    content_weight: Optional[float] = Query(None, ge=0.0, le=1.0),\n    structure_weight: Optional[float] = Query(None, ge=0.0, le=1.0),\n    semantic_weight: Optional[float] = Query(None, ge=0.0, le=1.0)\n):\n    \"\"\"\n    Update incremental scraping configuration\n    \"\"\"\n    try:\n        if similarity_threshold is not None:\n            incremental_scraper.similarity_threshold = similarity_threshold\n        \n        config_updates = {}\n        if content_weight is not None:\n            config_updates['content_weight'] = content_weight\n        if structure_weight is not None:\n            config_updates['structure_weight'] = structure_weight\n        if semantic_weight is not None:\n            config_updates['semantic_weight'] = semantic_weight\n        \n        # Ensure weights sum to 1.0\n        if config_updates:\n            current_config = incremental_scraper.change_detection_config.copy()\n            current_config.update(config_updates)\n            \n            total_weight = (\n                current_config.get('content_weight', 0) +\n                current_config.get('structure_weight', 0) +\n                current_config.get('semantic_weight', 0)\n            )\n            \n            if abs(total_weight - 1.0) > 0.01:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Weights must sum to 1.0, current sum: {total_weight}\"\n                )\n            \n            incremental_scraper.change_detection_config.update(config_updates)\n        \n        return {\n            \"message\": \"Configuration updated\",\n            \"similarity_threshold\": incremental_scraper.similarity_threshold,\n            \"config\": incremental_scraper.change_detection_config\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error updating config: {str(e)}\")\n\n@router.get(\"/similar-urls\")\nasync def find_similar_urls(\n    url: str,\n    similarity_threshold: float = Query(0.8, ge=0.0, le=1.0),\n    limit: int = Query(10, ge=1, le=50),\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    Find URLs with similar content based on semantic embeddings\n    \"\"\"\n    try:\n        # Get the embedding for the target URL\n        cache_entry = await incremental_scraper._get_cache_entry(url)\n        if not cache_entry:\n            raise HTTPException(status_code=404, detail=\"URL not found in cache\")\n        \n        target_embedding = cache_entry.semantic_embedding\n        \n        # Get all cached URLs\n        similar_urls = []\n        for cached_url, cached_entry in incremental_scraper.cache.items():\n            if cached_url == url:\n                continue\n            \n            similarity = incremental_scraper._calculate_cosine_similarity(\n                target_embedding,\n                cached_entry.semantic_embedding\n            )\n            \n            if similarity >= similarity_threshold:\n                similar_urls.append({\n                    \"url\": cached_url,\n                    \"similarity_score\": similarity,\n                    \"last_scraped\": cached_entry.last_scraped,\n                    \"job_id\": cached_entry.scraping_job_id\n                })\n        \n        # Sort by similarity score\n        similar_urls.sort(key=lambda x: x['similarity_score'], reverse=True)\n        \n        return {\n            \"target_url\": url,\n            \"similar_urls\": similar_urls[:limit],\n            \"total_found\": len(similar_urls)\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error finding similar URLs: {str(e)}\")\n"}}}