import os
import base64
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import google.generativeai as genai
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin, urlparse
import json
import re
from PIL import Image
import io
import pandas as pd
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ImageData:
    """Extracted image data with metadata"""
    url: str
    alt_text: str
    caption: str
    description: str  # Generated by Gemini Vision
    objects_detected: List[str]
    text_content: str  # OCR text from image
    context: str  # Surrounding HTML context
    dimensions: Tuple[int, int]
    file_size: int
    format: str
    confidence_score: float

@dataclass
class TableData:
    """Extracted table data with structure"""
    headers: List[str]
    rows: List[List[str]]
    caption: str
    summary: str  # Generated summary of table content
    data_types: Dict[str, str]  # Column data types
    relationships: List[str]  # Detected relationships between columns
    context: str  # Surrounding HTML context
    structure_quality: float
    completeness_score: float

@dataclass
class StructuredData:
    """Extracted structured data (JSON-LD, microdata, etc.)"""
    schema_type: str
    data: Dict[str, Any]
    source_format: str  # 'json-ld', 'microdata', 'rdfa', 'opengraph'
    confidence_score: float
    validation_errors: List[str]

@dataclass
class MultiModalExtractionResult:
    """Complete multi-modal extraction result"""
    url: str
    images: List[ImageData]
    tables: List[TableData]
    structured_data: List[StructuredData]
    text_content: str
    extraction_metadata: Dict[str, Any]
    processing_time: float
    total_confidence: float

class MultiModalExtractor:
    """Multi-modal data extraction pipeline using Gemini's vision capabilities"""
    
    def __init__(self):
        # Initialize Gemini models
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.vision_model = genai.GenerativeModel('gemini-pro-vision')
        self.text_model = genai.GenerativeModel('gemini-pro')
        
        # Cache directory for downloaded images
        self.cache_dir = Path("multimodal_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # Image processing settings
        self.max_image_size = (1024, 1024)
        self.supported_formats = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
        
    async def extract_multimodal_data(self, url: str, html_content: str) -> MultiModalExtractionResult:
        """Main extraction method for multi-modal data"""
        start_time = datetime.now()
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extract different types of data concurrently
            tasks = [
                self._extract_images(soup, url),
                self._extract_tables(soup),
                self._extract_structured_data(soup, html_content),
                self._extract_text_content(soup)
            ]
            
            images, tables, structured_data, text_content = await asyncio.gather(*tasks)
            
            # Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Calculate overall confidence
            total_confidence = self._calculate_total_confidence(images, tables, structured_data)
            
            # Generate extraction metadata
            metadata = {
                'extraction_timestamp': datetime.now().isoformat(),
                'images_count': len(images),
                'tables_count': len(tables),
                'structured_data_count': len(structured_data),
                'text_length': len(text_content),
                'processing_time_seconds': processing_time
            }
            
            return MultiModalExtractionResult(
                url=url,
                images=images,
                tables=tables,
                structured_data=structured_data,
                text_content=text_content,
                extraction_metadata=metadata,
                processing_time=processing_time,
                total_confidence=total_confidence
            )
            
        except Exception as e:
            logger.error(f"Error in multi-modal extraction for {url}: {str(e)}")
            return MultiModalExtractionResult(
                url=url,
                images=[],
                tables=[],
                structured_data=[],
                text_content="",
                extraction_metadata={'error': str(e)},
                processing_time=0.0,
                total_confidence=0.0
            )
    
    async def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[ImageData]:
        """Extract and analyze images using Gemini Vision"""
        images = []
        img_tags = soup.find_all('img')
        
        # Limit to first 10 images to avoid excessive API calls
        for img_tag in img_tags[:10]:
            try:
                src = img_tag.get('src')
                if not src:
                    continue
                    
                # Convert relative URLs to absolute
                img_url = urljoin(base_url, src)
                
                # Skip data URLs and very small images
                if img_url.startswith('data:') or self._is_small_image(img_tag):
                    continue
                
                # Download and analyze image
                image_data = await self._analyze_image_with_vision(img_url, img_tag, soup)
                if image_data:
                    images.append(image_data)
                    
            except Exception as e:
                logger.warning(f"Error processing image {img_url}: {str(e)}")
                continue
        
        return images
    
    async def _analyze_image_with_vision(self, img_url: str, img_tag, soup: BeautifulSoup) -> Optional[ImageData]:
        """Analyze image using Gemini Vision API"""
        try:
            # Download image
            response = requests.get(img_url, timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            response.raise_for_status()
            
            # Open and process image
            image = Image.open(io.BytesIO(response.content))
            
            # Resize if too large
            if image.size[0] > self.max_image_size[0] or image.size[1] > self.max_image_size[1]:
                image.thumbnail(self.max_image_size, Image.Resampling.LANCZOS)
            
            # Convert to RGB if necessary
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # Analyze with Gemini Vision
            vision_prompt = """
            Analyze this image and provide:
            1. A detailed description of what you see
            2. List of main objects/subjects in the image
            3. Any text content visible in the image (OCR)
            4. The likely purpose/context of this image
            
            Format your response as JSON:
            {
                "description": "detailed description",
                "objects": ["object1", "object2"],
                "text_content": "any visible text",
                "context": "likely purpose/context"
            }
            """
            
            response = self.vision_model.generate_content([vision_prompt, image])
            vision_result = self._parse_vision_response(response.text)
            
            # Extract metadata from HTML
            alt_text = img_tag.get('alt', '')
            caption = self._extract_image_caption(img_tag, soup)
            context = self._extract_image_context(img_tag, soup)
            
            return ImageData(
                url=img_url,
                alt_text=alt_text,
                caption=caption,
                description=vision_result.get('description', ''),
                objects_detected=vision_result.get('objects', []),
                text_content=vision_result.get('text_content', ''),
                context=context,
                dimensions=image.size,
                file_size=len(response.content),
                format=image.format or 'Unknown',
                confidence_score=0.8  # Base confidence, could be improved with validation
            )
            
        except Exception as e:
            logger.error(f"Error analyzing image {img_url}: {str(e)}")
            return None
    
    def _parse_vision_response(self, response_text: str) -> Dict[str, Any]:
        """Parse Gemini Vision response"""
        try:
            # Try to extract JSON from response
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        # Fallback parsing
        return {
            'description': response_text[:500],
            'objects': [],
            'text_content': '',
            'context': 'unknown'
        }
    
    async def _extract_tables(self, soup: BeautifulSoup) -> List[TableData]:
        """Extract and analyze tables"""
        tables = []
        table_tags = soup.find_all('table')
        
        for table_tag in table_tags:
            try:
                table_data = await self._analyze_table(table_tag, soup)
                if table_data:
                    tables.append(table_data)
            except Exception as e:
                logger.warning(f"Error processing table: {str(e)}")
                continue
        
        return tables
    
    async def _analyze_table(self, table_tag, soup: BeautifulSoup) -> Optional[TableData]:
        """Analyze individual table"""
        try:
            # Extract table structure
            headers = []
            rows = []
            
            # Find headers
            header_row = table_tag.find('tr')
            if header_row:
                header_cells = header_row.find_all(['th', 'td'])
                headers = [cell.get_text(strip=True) for cell in header_cells]
            
            # Extract data rows
            data_rows = table_tag.find_all('tr')[1:] if headers else table_tag.find_all('tr')
            for row in data_rows:
                cells = row.find_all(['td', 'th'])
                row_data = [cell.get_text(strip=True) for cell in cells]
                if row_data:  # Skip empty rows
                    rows.append(row_data)
            
            if not rows:
                return None
            
            # Generate table summary using Gemini
            table_text = self._table_to_text(headers, rows)
            summary = await self._generate_table_summary(table_text)
            
            # Analyze data types and relationships
            data_types = self._analyze_column_types(headers, rows)
            relationships = await self._detect_column_relationships(headers, rows)
            
            # Extract context
            caption = self._extract_table_caption(table_tag, soup)
            context = self._extract_table_context(table_tag, soup)
            
            # Calculate quality scores
            structure_quality = self._calculate_table_structure_quality(headers, rows)
            completeness_score = self._calculate_table_completeness(rows)
            
            return TableData(
                headers=headers,
                rows=rows,
                caption=caption,
                summary=summary,
                data_types=data_types,
                relationships=relationships,
                context=context,
                structure_quality=structure_quality,
                completeness_score=completeness_score
            )
            
        except Exception as e:
            logger.error(f"Error analyzing table: {str(e)}")
            return None
    
    async def _generate_table_summary(self, table_text: str) -> str:
        """Generate table summary using Gemini"""
        try:
            prompt = f"""
            Analyze this table data and provide a concise summary of:
            1. What the table contains
            2. Key patterns or insights
            3. The main purpose of this data
            
            Table data:
            {table_text[:2000]}
            
            Provide a summary in 2-3 sentences.
            """
            
            response = self.text_model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"Error generating table summary: {str(e)}")
            return "Table summary unavailable"
    
    async def _extract_structured_data(self, soup: BeautifulSoup, html_content: str) -> List[StructuredData]:
        """Extract structured data (JSON-LD, microdata, etc.)"""
        structured_data = []
        
        # Extract JSON-LD
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                structured_data.append(StructuredData(
                    schema_type=data.get('@type', 'Unknown'),
                    data=data,
                    source_format='json-ld',
                    confidence_score=0.9,
                    validation_errors=[]
                ))
            except Exception as e:
                logger.warning(f"Error parsing JSON-LD: {str(e)}")
        
        # Extract Open Graph data
        og_data = self._extract_opengraph_data(soup)
        if og_data:
            structured_data.append(StructuredData(
                schema_type='OpenGraph',
                data=og_data,
                source_format='opengraph',
                confidence_score=0.8,
                validation_errors=[]
            ))
        
        # Extract microdata
        microdata = self._extract_microdata(soup)
        for item in microdata:
            structured_data.append(StructuredData(
                schema_type=item.get('type', 'Unknown'),
                data=item,
                source_format='microdata',
                confidence_score=0.7,
                validation_errors=[]
            ))
        
        return structured_data
    
    def _extract_opengraph_data(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """Extract Open Graph metadata"""
        og_data = {}
        og_tags = soup.find_all('meta', property=lambda x: x and x.startswith('og:'))
        
        for tag in og_tags:
            property_name = tag.get('property', '').replace('og:', '')
            content = tag.get('content', '')
            if property_name and content:
                og_data[property_name] = content
        
        return og_data if og_data else None
    
    def _extract_microdata(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """Extract microdata from HTML"""
        microdata_items = []
        items = soup.find_all(attrs={'itemscope': True})
        
        for item in items:
            item_data = {
                'type': item.get('itemtype', ''),
                'properties': {}
            }
            
            # Extract properties
            props = item.find_all(attrs={'itemprop': True})
            for prop in props:
                prop_name = prop.get('itemprop')
                prop_value = prop.get('content') or prop.get_text(strip=True)
                if prop_name and prop_value:
                    item_data['properties'][prop_name] = prop_value
            
            if item_data['properties']:
                microdata_items.append(item_data)
        
        return microdata_items
    
    async def _extract_text_content(self, soup: BeautifulSoup) -> str:
        """Extract clean text content"""
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text content
        text = soup.get_text(separator=' ', strip=True)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    # Helper methods
    def _is_small_image(self, img_tag) -> bool:
        """Check if image is too small to be meaningful"""
        width = img_tag.get('width')
        height = img_tag.get('height')
        
        if width and height:
            try:
                w, h = int(width), int(height)
                return w < 50 or h < 50
            except ValueError:
                pass
        
        return False
    
    def _extract_image_caption(self, img_tag, soup: BeautifulSoup) -> str:
        """Extract image caption from surrounding elements"""
        # Look for figcaption
        figure = img_tag.find_parent('figure')
        if figure:
            figcaption = figure.find('figcaption')
            if figcaption:
                return figcaption.get_text(strip=True)
        
        # Look for nearby caption elements
        for sibling in img_tag.find_next_siblings(['p', 'div', 'span']):
            if 'caption' in sibling.get('class', []):
                return sibling.get_text(strip=True)
        
        return ''
    
    def _extract_image_context(self, img_tag, soup: BeautifulSoup) -> str:
        """Extract surrounding context for image"""
        context_parts = []
        
        # Get parent element text
        parent = img_tag.find_parent(['div', 'section', 'article'])
        if parent:
            # Get text from siblings
            for sibling in parent.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                text = sibling.get_text(strip=True)
                if text and len(text) > 10:
                    context_parts.append(text)
                    if len(context_parts) >= 3:
                        break
        
        return ' '.join(context_parts)[:500]
    
    def _table_to_text(self, headers: List[str], rows: List[List[str]]) -> str:
        """Convert table to text representation"""
        text_parts = []
        
        if headers:
            text_parts.append('Headers: ' + ', '.join(headers))
        
        for i, row in enumerate(rows[:5]):  # Limit to first 5 rows
            text_parts.append(f'Row {i+1}: ' + ', '.join(row))
        
        return '\n'.join(text_parts)
    
    def _analyze_column_types(self, headers: List[str], rows: List[List[str]]) -> Dict[str, str]:
        """Analyze data types of table columns"""
        data_types = {}
        
        if not headers or not rows:
            return data_types
        
        for i, header in enumerate(headers):
            column_values = [row[i] if i < len(row) else '' for row in rows]
            data_types[header] = self._detect_column_type(column_values)
        
        return data_types
    
    def _detect_column_type(self, values: List[str]) -> str:
        """Detect the data type of a column"""
        non_empty_values = [v for v in values if v.strip()]
        
        if not non_empty_values:
            return 'empty'
        
        # Check for numeric
        numeric_count = 0
        date_count = 0
        
        for value in non_empty_values:
            # Check if numeric
            try:
                float(value.replace(',', '').replace('$', '').replace('%', ''))
                numeric_count += 1
            except ValueError:
                pass
            
            # Check if date-like
            if re.search(r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\d{4}-\d{2}-\d{2}', value):
                date_count += 1
        
        total = len(non_empty_values)
        
        if numeric_count / total > 0.8:
            return 'numeric'
        elif date_count / total > 0.8:
            return 'date'
        else:
            return 'text'
    
    async def _detect_column_relationships(self, headers: List[str], rows: List[List[str]]) -> List[str]:
        """Detect relationships between columns using Gemini"""
        try:
            if len(headers) < 2:
                return []
            
            # Create sample data for analysis
            sample_data = self._table_to_text(headers, rows[:3])
            
            prompt = f"""
            Analyze this table structure and identify potential relationships between columns:
            
            {sample_data}
            
            Look for relationships like:
            - Foreign key relationships
            - Calculated fields
            - Hierarchical relationships
            - Categorical groupings
            
            Return a list of relationship descriptions, one per line.
            """
            
            response = self.text_model.generate_content(prompt)
            relationships = [line.strip() for line in response.text.split('\n') if line.strip()]
            
            return relationships[:5]  # Limit to 5 relationships
            
        except Exception as e:
            logger.error(f"Error detecting column relationships: {str(e)}")
            return []
    
    def _extract_table_caption(self, table_tag, soup: BeautifulSoup) -> str:
        """Extract table caption"""
        caption = table_tag.find('caption')
        if caption:
            return caption.get_text(strip=True)
        
        # Look for preceding heading
        for sibling in table_tag.find_previous_siblings(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            return sibling.get_text(strip=True)
        
        return ''
    
    def _extract_table_context(self, table_tag, soup: BeautifulSoup) -> str:
        """Extract table context"""
        context_parts = []
        
        # Get surrounding paragraphs
        for sibling in table_tag.find_previous_siblings(['p']):
            text = sibling.get_text(strip=True)
            if text:
                context_parts.append(text)
                break
        
        for sibling in table_tag.find_next_siblings(['p']):
            text = sibling.get_text(strip=True)
            if text:
                context_parts.append(text)
                break
        
        return ' '.join(context_parts)[:300]
    
    def _calculate_table_structure_quality(self, headers: List[str], rows: List[List[str]]) -> float:
        """Calculate table structure quality score"""
        if not headers or not rows:
            return 0.0
        
        score = 0.0
        
        # Check header quality
        if headers and all(h.strip() for h in headers):
            score += 0.3
        
        # Check row consistency
        if rows:
            expected_cols = len(headers) if headers else len(rows[0])
            consistent_rows = sum(1 for row in rows if len(row) == expected_cols)
            score += 0.4 * (consistent_rows / len(rows))
        
        # Check for empty cells
        total_cells = sum(len(row) for row in rows)
        empty_cells = sum(1 for row in rows for cell in row if not cell.strip())
        if total_cells > 0:
            score += 0.3 * (1 - empty_cells / total_cells)
        
        return min(score, 1.0)
    
    def _calculate_table_completeness(self, rows: List[List[str]]) -> float:
        """Calculate table completeness score"""
        if not rows:
            return 0.0
        
        total_cells = sum(len(row) for row in rows)
        filled_cells = sum(1 for row in rows for cell in row if cell.strip())
        
        return filled_cells / total_cells if total_cells > 0 else 0.0
    
    def _calculate_total_confidence(self, images: List[ImageData], tables: List[TableData], structured_data: List[StructuredData]) -> float:
        """Calculate overall extraction confidence"""
        scores = []
        
        # Image confidence scores
        if images:
            avg_image_confidence = sum(img.confidence_score for img in images) / len(images)
            scores.append(avg_image_confidence)
        
        # Table quality scores
        if tables:
            avg_table_quality = sum(table.structure_quality for table in tables) / len(tables)
            scores.append(avg_table_quality)
        
        # Structured data confidence
        if structured_data:
            avg_structured_confidence = sum(data.confidence_score for data in structured_data) / len(structured_data)
            scores.append(avg_structured_confidence)
        
        return sum(scores) / len(scores) if scores else 0.0